
#### Obsidian 主流AI插件功能对比

| 插件名称 / 功能                | 语义搜索 | 智能关联/推荐 | 与整个知识库对话 | 与当前笔记对话 | 侧边栏聊天 | 在正文中生成/修改 | 自定义模板/提示词 | 内置快捷命令 |
| :----------------------- | :--: | :-----: | :------: | :-----: | :---: | :-------: | :-------: | :----: |
| **Smart Connections**    |  ✔️  |   ✔️    |    ✔️    |   ✔️    |  ✔️   |     ❌     |     ❌     |   ❌    |
| **Copilot for Obsidian** |  ✔️  |    ❌    |    ✔️    |   ✔️    |  ✔️   |     ❌     |    ✔️     |   ✔️   |
| **Text Generator**       |  ❌   |    ❌    |    ✔️    |   ✔️    |  ✔️   |    ✔️     |    ✔️     |   ✔️   |
| **Smart Composer**       |  ❌   |    ❌    |    ❌     |   ✔️    |  ✔️   |    ✔️     |     ❌     |   ✔️   |


## 如何选择与组合
*   **如果你想盘活已有知识，发现隐藏的连接：**
    *   **首选 `Smart Connections`**。它的核心优势是作为“知识发现者”，通过语义关联推荐笔记，这是其他插件无法替代的。

*   **如果你主要用Obsidian写作：**
    *   **深度创作者选 `Text Generator`**。它如同“写作大师”，能深度融入你的写作流，直接在正文中生成大纲、续写段落，并支持高度自定义的模板。
    *   **追求快速编辑选 `Smart Composer`**。它是一个“效率编辑”，专注于快速修改、润色和重写你选中的文字，响应迅速。

*   **如果你需要一个全能的AI助手：**
    *   **选择 `Copilot for Obsidian`**。它是一个“全能管家”，功能全面，尤其擅长与整个知识库进行问答和执行各种预设命令。

*   **高效组合工作流建议：**
    1.  **研究构思**：使用 `Smart Connections` 发现相关笔记，建立知识联系。
    2.  **起草内容**：使用 `Text Generator` 生成大纲和初稿。
    3.  **润色定稿**：使用 `Smart Composer` 对具体段落进行快速修改和优化。



## AI连接方式：云端API vs. 本地模型

所有主流AI插件都支持以下两种连接方式，根据自己的需求、成本预算和隐私考量进行选择。

| 对比项 | 云端 API (Cloud API) | 本地大模型 (Local LLM) |
| :--- | :--- | :--- |
| **工作原理** | 插件通过互联网将你的请求发送给AI服务商（如OpenAI），接收并返回结果。 | 插件与你在自己电脑上运行的AI模型直接通信，所有数据处理均在本地完成。 |
| **配置方式** | 在插件设置中，粘贴你从AI服务商处获取的**API Key**。 | 在插件设置中，将API地址指向**本地服务地址**（如 `http://localhost:11434`）。 |
| **收费模式** | **按量付费**。插件本身不收费，费用由API服务商（如OpenAI）根据你的使用量收取。 | **无API调用费用**。成本主要在于一次性的硬件投入（如果需要升级）和电费。 |
| **优点** | ✅ 性能强大（可使用最新模型）<br>✅ 无需本地硬件配置<br>✅ 设置简单快捷 | ✅ 极致的数据隐私与安全<br>✅ 无持续性API费用<br>✅ 可完全离线使用 |
| **缺点** | ❌ 有持续的费用支出<br>❌ 数据需发送到第三方服务器（有隐私考量） | ❌ 对电脑硬件（显卡、内存）有要求<br>❌ 模型性能可能弱于顶级云端模型<br>❌ 需要一定的初始安装和配置 |

### 本地大模型（Local LLM）简要部署流程

如果你对本地模型感兴趣，可以参考下表的三个核心步骤进行部署。以**Ollama**（推荐）为例。

| 步骤 | 核心工具 | 关键操作 |
| :--- | :--- | :--- |
| **第一步** | **Ollama官网** | 前往Ollama官网，根据你的操作系统（Windows/macOS/Linux）下载并安装它。 |
| **第二步** | **终端/命令行** | 打开终端，输入命令 `ollama run llama3` 来下载并运行一个模型。这会自动在本地启动一个服务。 |
| **第三步** | **Obsidian插件设置** | 在AI插件的设置中，找到服务器地址（Base URL），填入本地地址 `http://localhost:11434` 并保存。 |